{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "733c68e5",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "- Here, first we'll focus on performing both Uni-Variate and Bi-Variate analysis on the given features\n",
    "- Secondly, we shall focus on finding the most used words using Wordcloud\n",
    "- Finally, we shall perform Sentiment Analysis on the reviews using NLTK and AFINN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e01e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from the csv file\n",
    "\n",
    "df = pd.read_csv('BA_reviews_forage_clean.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8defa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bf5788",
   "metadata": {},
   "source": [
    "# Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734ef4b3",
   "metadata": {},
   "source": [
    "# Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6618bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Overall Rating using Boxplot\n",
    "\n",
    "plt.figure(figsize=(3,4))\n",
    "sns.boxplot(y = df['stars'])\n",
    "plt.title('Overall Average Rating for BA')\n",
    "plt.ylabel('Rating')\n",
    "plt.xlabel('Stars')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ca1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Overall Average Rating for BA based on all reviews = ',round(df['stars'].mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcca674",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "ax = sns.barplot(data=df, y = df['stars'].value_counts().values, x = df['stars'].value_counts().index)\n",
    "plt.title('Count of Each Star Rating')\n",
    "plt.xlabel('Number of Stars')\n",
    "plt.ylabel('Count')\n",
    "plt.bar_label(ax.containers[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5871da3",
   "metadata": {},
   "source": [
    "- We can observe that most ratings given are 1 star\n",
    "- 2 and 3 star reviews occupy a good percentage\n",
    "- 6 star rating is the lowest by count, given by the customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021655e6",
   "metadata": {},
   "source": [
    "# Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11533772",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "ax = sns.barplot(data=df, y = df['country'].value_counts().head(20).values, x = df['country'].value_counts().head(20).index)\n",
    "plt.title('Top 20 countries by number of reviews')\n",
    "plt.xlabel('Country Name')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.bar_label(ax.containers[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dabbecb",
   "metadata": {},
   "source": [
    "- United Kingdom has the most reviews coming from. It co-relates with the fact that BA operates from UK as the base as the company is based from UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f592a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "ax = sns.barplot(data=df, y = df['country'].value_counts().tail(20).values, x = df['country'].value_counts().tail(20).index)\n",
    "plt.title('Top 20 countries with least number of reviews')\n",
    "plt.xlabel('Country Name')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.bar_label(ax.containers[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16657a38",
   "metadata": {},
   "source": [
    "# Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4920d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'].dt.strftime('%B').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f7609",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "df['date'].dt.month.value_counts().sort_index(ascending=True).plot(kind='line',marker = '*')\n",
    "plt.title('Total number of reviews per month with respect to all years')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167461eb",
   "metadata": {},
   "source": [
    "- June(6) and October(8) seems to be the time when most people travel in BA\n",
    "- It can be attributed to the fact that it is a time period where a lot of people travel to spend their holidays\n",
    "- On the contrary, low review counts on months like Feb and March might also mean that the service from BA was either not too great or too bad enough that customers did not bother about giving their reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958ceb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,5))\n",
    "ax = sns.countplot(x = df['date'].dt.year)\n",
    "plt.title('Number of reviews per year')\n",
    "plt.xlabel('Year')\n",
    "plt.bar_label(ax.containers[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153557fd",
   "metadata": {},
   "source": [
    "- Low count of reviews in years 2011 and 2012 can be related to the fact that there is very less data present for that time period\n",
    "- Years 2020 and 2021 saw a huge drop which could be related to the rise of Covid-19 pandemic due to which all international passenger services were stopped for most part of the year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375a0b4",
   "metadata": {},
   "source": [
    "# Bivariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90cac91",
   "metadata": {},
   "source": [
    "# Country vs Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a9a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = pd.DataFrame(df.groupby('country').mean()['stars'].sort_values(ascending=False)).reset_index()\n",
    "dg.rename(columns={'stars':'avg_rating'}, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(18,5))\n",
    "ax = sns.barplot(x='country', y='avg_rating', data=dg.head(15))\n",
    "plt.title(\"Top 15 Countries with Highest average rating provided to BA\")\n",
    "plt.xticks(rotation = 45)\n",
    "plt.bar_label(ax.containers[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d76a81",
   "metadata": {},
   "source": [
    "- Most of the very high rating can be attributed to the fact of having low count of reviews from that country\n",
    "- By havng a look at the plots in Uni-Variate 'Country' analysis, we can see that countries like Oman, Chille, Costa Rica etc. have only about 1-2 reviews on the whole.\n",
    "- Similarly for the plot below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbcb720",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = pd.DataFrame(df.groupby('country').mean()['stars'].sort_values(ascending=False)).reset_index()\n",
    "dg.rename(columns={'stars':'avg_rating'}, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(18,5))\n",
    "ax = sns.barplot(x='country', y='avg_rating', data=dg.tail(15))\n",
    "plt.title(\"Top 15 Countries with Lowest average rating provided to BA\")\n",
    "plt.xticks(rotation = 45)\n",
    "plt.bar_label(ax.containers[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52429a58",
   "metadata": {},
   "source": [
    "# Date vs Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9efe4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "ax = sns.countplot(hue = df['stars'], x = df['date'].dt.year)\n",
    "plt.title('Share of each star rating for each year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count of each star rating')\n",
    "plt.bar_label(ax.containers[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3311ea",
   "metadata": {},
   "source": [
    "- This plot shows us that, 1 star rating has the most shares throught all the years.\n",
    "- Peak 1 star reviews are in year 2017. Actions taken that year by the management must be scrutinized properly\n",
    "- 2023 should also be looked into, crossing 6 months yet majority ratings are 1 star. BA should work on the current feedbacks ASAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d74720",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "ax = sns.barplot(y = df['stars'], x = df['date'].dt.year)\n",
    "plt.title('Average Star Rating per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Star Rating')\n",
    "plt.bar_label(ax.containers[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce5631d",
   "metadata": {},
   "source": [
    "- We can confirm that high star rating in 2011 is due to low count of entries at that time\n",
    "- The year with the most entries is 2015 which also has the 3rd highest overall rating for a year\n",
    "- 2023 rating is bad with 3.35, eventhough its been only 6 months till now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c05499",
   "metadata": {},
   "source": [
    "* Ratings in different time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throught the whole period of the data according to each month\n",
    "\n",
    "df.groupby(df['date'].dt.month)['stars'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad7eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,2))\n",
    "sns.barplot(x = df['date'].dt.strftime('%B'), y = df['stars'])\n",
    "plt.title('Start ratings per month throught the timeline')\n",
    "plt.xticks(ticks= [0,1,2,3,4,5,6,7,8,9,10,11], rotation = 45, )\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Star Rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c06e3ff",
   "metadata": {},
   "source": [
    "- The month with the most positive ratings is November\n",
    "- The month with the least positive rating is June"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191e7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-order dataframe according to date from past to present\n",
    "\n",
    "dg = df.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,3))\n",
    "sns.lineplot(x = dg['date'].dt.strftime('%Y'), y = dg['stars'], markers='*')\n",
    "plt.title('Trend')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average rating per Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d329c",
   "metadata": {},
   "source": [
    "- The time period between 2017 to 2019 shows that BA did work on the feebacks and improve, but only till 2020.\n",
    "- The downward line plot from 2020 to 2023 shows a pretty degrading trend for the company since Covid-19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df, x='date', y=\"stars\")\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3af2e5",
   "metadata": {},
   "source": [
    "- An advance plot to help look with more detail about the number of reviews and it's ratings in a time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7796b2",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce81298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Start with one review:\n",
    "reviews = \" \".join(df.corpus)\n",
    "\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d4a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(height=600,width=800,max_font_size=100, max_words=500, stopwords=stopwords).generate(reviews)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8744541",
   "metadata": {},
   "source": [
    "- In the above wordcloud, we observe some words like \"aircraft\", \"could\", \"even\" that doesn't give an idea of whether a review is positive of negative.\n",
    "- So, we remove them by adding them into the list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c01178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "reviews = \" \".join(df.corpus)\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stopwords.update([\"ba\", \"flight\", \"british\", \"airway\", \"airways\" \"airline\", \"plane\", \"told\", \"also\", \"passenger\", 'business', \n",
    "                  'class', \"london\", \"heathrow\", \"aircraft\", \"could\", \"even\", \"would\", 'fly', 'trip', 'verify'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e3795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(height=600,width=800,max_font_size=100, max_words=300, stopwords=stopwords).generate(reviews)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253f222",
   "metadata": {},
   "source": [
    "# Word Frequency with Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e4d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#split the text of all reviews into a list of words\n",
    "words = reviews.split(\" \")\n",
    "\n",
    "#remove certain words that will not be used to determine the positive or negative sentiment\n",
    "stopwords = text.ENGLISH_STOP_WORDS.union(['flight', 'ba', \"passenger\", \"u\", \"london\", \"airway\", \"british\", \"airline\",\n",
    "                                           \"heathrow\",\"plane\", \"lhr\",\"review\", 'I', 'fly', 'trip','verify'])\n",
    "\n",
    "\n",
    "new_words = [word for word in words if word not in stopwords]\n",
    "\n",
    "nlp_words=FreqDist(new_words).most_common(20)\n",
    "\n",
    "#create a dataframe of these word and its frequencies\n",
    "all_fdist = pd.Series(dict(nlp_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e8518",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Setting figure, ax into variables\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "## Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n",
    "all_plot = sns.barplot(x=all_fdist.index, y=all_fdist.values, ax=ax)\n",
    "all_plot.bar_label(all_plot.containers[0])\n",
    "plt.title('Frequently used words')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b391ecba",
   "metadata": {},
   "source": [
    "- Seats, service, time are some of the most frequently found words.\n",
    "- These words being used so many times could eith mean something praise worthy or totally bad about what they refer to\n",
    "- We shall further split these words catogorizing them based on the ratings these reviews were associated with.\n",
    "- By doing so, we would get a better idea and context about the use of those words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fbad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.collocations as collocations\n",
    "from nltk import FreqDist, bigrams\n",
    "\n",
    "reviews = \" \".join(df.corpus)\n",
    "\n",
    "#split the text of all reviews into a list of words\n",
    "words = reviews.split(\" \")\n",
    "\n",
    "new_words = [word for word in words if word not in stopwords]\n",
    "\n",
    "def get_freq_dist(new_words,number_of_ngrams ):\n",
    "    from nltk import ngrams\n",
    "    \n",
    "    ## Generate bigrams\n",
    "    ngrams = ngrams(new_words, number_of_ngrams)\n",
    "\n",
    "    ## Creating FreqDist\n",
    "    ngram_fd = FreqDist(ngrams).most_common(40)\n",
    "\n",
    "    ## Sort values by highest frequency\n",
    "    ngram_sorted = {k:v for k,v in sorted(ngram_fd, key=lambda item:item[1])}\n",
    "\n",
    "    ## Join bigram tokens with '_' + maintain sorting\n",
    "    ngram_joined = {'_'.join(k):v for k,v in sorted(ngram_fd, key=lambda item:item[1])}\n",
    "\n",
    "    ## Convert to Pandas series for easy plotting\n",
    "    ngram_freqdist = pd.Series(ngram_joined)\n",
    "    plt.figure(figsize=(10,9))\n",
    "    ax = ngram_freqdist.plot(kind=\"barh\")\n",
    "    \n",
    "    return ax\n",
    "\n",
    "get_freq_dist(new_words,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e60eeb6",
   "metadata": {},
   "source": [
    "* Seperating the word frequency based on the 'Star rating' given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4139da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_ratings = df[df['stars'].isin([1,2,3])]\n",
    "avg_ratings = df[df['stars'].isin([4,5,6])]\n",
    "good_ratings = df[df['stars'].isin([7,8,9,10])]\n",
    "\n",
    "bad_review = \" \".join(bad_ratings['corpus'])\n",
    "avg_review = \" \".join(avg_ratings['corpus'])\n",
    "good_review = \" \".join(good_ratings['corpus'])\n",
    "\n",
    "#split the text of all reviews into a list of words\n",
    "bad_remarks = bad_review.split(\" \")\n",
    "avg_remarks = avg_review.split(\" \")\n",
    "good_remarks = good_review.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b96bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_good_words = [word for word in good_remarks if word not in stopwords]\n",
    "\n",
    "get_freq_dist(new_good_words,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e690f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_avg_words = [word for word in avg_remarks if word not in stopwords]\n",
    "\n",
    "get_freq_dist(new_avg_words,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bad_words = [word for word in bad_remarks if word not in stopwords]\n",
    "\n",
    "get_freq_dist(new_bad_words,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400cdd03",
   "metadata": {},
   "source": [
    "- The above 3 plots gives us an average idea about what these words mean together based on the ratings the customer gave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b12f8",
   "metadata": {},
   "source": [
    "# Analysing Sentiment using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ef739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc4fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e51e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vds = SentimentIntensityAnalyzer()\n",
    "\n",
    "#set a column Polarity with all 0 values initially\n",
    "df['label'] = 0\n",
    "    \n",
    "for i in range(len(df.corpus)):\n",
    "    \n",
    "    score = vds.polarity_scores(df.corpus[i])['compound']\n",
    "    #print(score)\n",
    "    if score > 0.2:\n",
    "        df['label'][i] = 1\n",
    "        #print(\"1st\")\n",
    "    elif score < 0:\n",
    "        df['label'][i] = -1\n",
    "        #print(\"2nd\")\n",
    "    else:\n",
    "        df['label'][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e047806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd07b11",
   "metadata": {},
   "source": [
    "- A score of 1 indicates a high degree of positive sentiment in the analyzed text. It implies that the text contains predominantly positive words and conveys a positive overall sentiment.\n",
    "- A score of -1 indicates a high degree of negative sentiment in the analyzed text. It suggests that the text contains predominantly negative words and conveys a negative overall sentiment.\n",
    "- A score of 0 suggests that the analyzed text has a neutral sentiment. It means that the text doesn't strongly express either positive or negative emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c09a949",
   "metadata": {},
   "source": [
    "# Analysing Sentiment using Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9367d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "#set a column Polarity with all 0 values initially\n",
    "df['polarity'] = 0\n",
    "\n",
    "for i in range(len(df.corpus)):\n",
    "    sent= TextBlob(df.corpus[i])\n",
    "    polarity  = sent.sentiment.polarity\n",
    "    subjectivity  = sent.sentiment.subjectivity\n",
    "    df['polarity'][i] = polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedb3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{df[(df['polarity'] >-0.2) & (df['polarity'] <0.2)].shape[0]} number of reviews between -0.2 and 0.2 polarity score\")\n",
    "\n",
    "print(f\"{df[(df['polarity'] >-0.1) & (df['polarity'] <0.1)].shape[0]} number of reviews between -0.1 and 0.1 polarity score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9991b8",
   "metadata": {},
   "source": [
    "- Polarity score is given between -0.1 to 0.1.\n",
    "- Closer the value to -0.1, indicates negative review \n",
    "- Closer the value to 0.1, indicates positive value. \n",
    "- If we consider a threshold where any review with polarity greater than 0.2 is positive and less than -0.2 is negative, we are left with 2286 reviews that lies in the neutral zone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#create an object of count vectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "#apply transformation\n",
    "tf = vect.fit_transform(df.corpus).toarray()\n",
    "tf_feature_names = vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc61c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#declare the number of topics\n",
    "number_of_topics = 8\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)\n",
    "\n",
    "#fit the term frequency data to the model\n",
    "model.fit(tf)\n",
    "\n",
    "#create empty dictionary to store key value pair of topic number and its weights\n",
    "topic_dict = {}\n",
    "\n",
    "#loop through model components \n",
    "for topic_idx, topic in enumerate(model.components_):\n",
    "    topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(tf_feature_names[i])\n",
    "            for i in topic.argsort()[:-10 - 1:-1]]\n",
    "    topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "            for i in topic.argsort()[:-10 - 1:-1]]\n",
    "    \n",
    "df_topic =pd.DataFrame(topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ca7423",
   "metadata": {},
   "source": [
    "# Sentiment Analysis usinf AFINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23078ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "af = Afinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065611fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = [af.score(text) for text in corpus]\n",
    "\n",
    "sentiment_category = ['positive' if score > 0 \n",
    "                          else 'negative' if score < 0 \n",
    "                              else 'neutral' \n",
    "                                  for score in sentiment_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b452e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.DataFrame([list(df['country']), sentiment_scores, sentiment_category]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b42a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.columns = ['country', 'sentiment_score', 'sentiment_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2166a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['sentiment_score'] = dfs.sentiment_score.astype('float')\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b9851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.groupby(by=['country']).describe().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cad8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "sp = sns.stripplot(x='country', y=\"sentiment_score\",  hue='country', data=dfs.head(30), ax=ax1)\n",
    "bp = sns.boxplot(x='country', y=\"sentiment_score\", hue='country', data=dfs.head(30), palette=\"Set2\", ax=ax2)\n",
    "plt.xticks(rotation =45)\n",
    "t = f.suptitle('Visualizing Sentiment', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"country\", hue=\"sentiment_category\", data=dfs.head(50), kind = 'strip', palette={\"negative\": \"#FE2020\", \n",
    "                             \"positive\": \"#BADD07\", \n",
    "                             \"neutral\": \"#68BFF5\"})\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37104591",
   "metadata": {},
   "source": [
    "# Most positive and negative review as per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab84cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[dfs['country'] =='United Kingdom']['sentiment_score'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9593f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_idx = dfs[(dfs['country']=='United Kingdom') & (dfs.sentiment_score == 72)].index[0]\n",
    "\n",
    "neg_idx = dfs[(dfs['country']=='United Kingdom') & (dfs.sentiment_score == -36)].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f092455",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most Negative Review  \\n', df.iloc[neg_idx][['reviews']][0], '\\t')\n",
    "print()\n",
    "print('Most Positive Review  \\n ',df.iloc[pos_idx][['reviews']][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc1ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
